{
  "agent_id": "coder3",
  "task_id": "task_3",
  "files": [
    {
      "name": "memory.py",
      "purpose": "Experience replay and memory",
      "priority": "medium"
    },
    {
      "name": "reward_system.py",
      "purpose": "Reward calculation and shaping",
      "priority": "medium"
    }
  ],
  "project_info": {
    "project_name": "enhanced_stat.ML_2508.15678v1_Tree_like_Pairwise_Interaction_Networks",
    "project_type": "agent",
    "description": "Enhanced AI project based on stat.ML_2508.15678v1_Tree-like-Pairwise-Interaction-Networks with content analysis. Detected project type: agent (confidence score: 6 matches).",
    "key_algorithms": [
      "Tion",
      "Actuarial",
      "Given",
      "Improving",
      "Machine",
      "Selected",
      "Optimal",
      "All",
      "Enhances",
      "Additive"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: stat.ML_2508.15678v1_Tree-like-Pairwise-Interaction-Networks.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nTree-like Pairwise Interaction Networks\nRonald Richman\u2217Salvatore Scognamiglio\u2020Mario V. W\u00a8 uthrich\u2021\nAugust 22, 2025\nAbstract\nModeling feature interactions in tabular data remains a key challenge in predictive mod-\neling, for example, as used for insurance pricing. This paper proposes the Tree-like Pair-\nwise Interaction Network (PIN), a novel neural network architecture that explicitly captures\npairwise feature interactions through a shared feed-forward neural network architecture that\nmimics the structure of decision trees. PIN enables intrinsic interpretability by design, al-\nlowing for direct inspection of interaction effects. Moreover, it allows for efficient SHapley\u2019s\nAdditive exPlanation (SHAP) computations because it only involves pairwise interactions.\nWe highlight connections between PIN and established models such as GA \u00b2Ms, gradient\nboosting machines, and graph neural networks. Empirical results on the popular French\nmotor insurance dataset show that PIN outperforms both traditional and modern neural\nnetworks benchmarks in predictive accuracy, while also providing insight into how features\ninteract with each another and how they contribute to the predictions.\nKeywords. Regression modeling, insurance pricing, tabular data, generalized linear models,\ngeneralized additive models, neural networks, Shapley explanation, interaction effects.\n1 Introduction\nUnderstanding and modeling the interactions among input features represents a core challenge\nin predictive modeling on tabular data. These interactions refer to how multiple variables jointly\ninfluence the target outcome in ways that go beyond their individual, isolated effects. In many\nreal-world cases, the relationship between predictors and the response variable is not simply\nadditive and may involve complex interdependencies. This is particularly relevant in insurance\npricing, where factors such as driver age, location, and driving behavior can interact in non-\nobvious ways to affect risk assessment and premium calculation. Overlooking or misspecifying\nsuch interactions can lead to suboptimal models, resulting in price distortions and potentially\nbiased interpretations.\nTraditionally, the insurance industry has relied on Generalized Linear Models (GLMs) due to\ntheir interpretability, strong statistical foundation and also for computational reasons; we refer\nto Nelder\u2013Wedderburn [22] and McCullagh\u2013Nelder [21]. In the GLM framework, the expected\nvalue of the response variable is modeled as a linear combination of predictors, linked to the\n\u2217InsureAI and University of the Witwatersrand, ronaldrichman@gmail.com\n\u2020Department of Management and Quantitative Sciences, University of Naples \u201cParthenope\u201d,\nsalvatore.scognamiglio@uniparthenope.it\n\u2021Department of Mathematics, ETH Zurich, mario.wuethrich@math.ethz.ch\n1arXiv:2508.15678v1  [stat.ML]  21 Aug 2025\n\n--- Page 2 ---\nresponse through a specified link function. While GLMs allow for the estimation of individual\ncovariate effects, they offer limited capability to capture interactions among variables. In fact,\ninteractions can be integrated, but each one needs to be engineered manually which clearly limits\ntheir scope and the model complexity and accuracy.\nGeneralized Additive Models (GAMs) were introduced in Hastie\u2013Tibshirani [12] as an extension\nof GLMs relaxing the assumption of linearity. They allow each predictor to have a nonlin-\near effect on the response variable. This is accomplished by replacing the original covariates\nwith smooth functions (typically splines) applied individually to each input variable. GAMs\nhave often been used in the context of actuarial science as a foundation for developing more\nsophisticated methods to solve actuarial problems; notable examples include Chang et al. [4]\nand Maillart\u2013Robert [18]. Building upon GAMs, Generalized Additive Models with Pairwise\nInteractions (GA \u00b2Ms) extend the framework by incorporating a selected set of pairwise feature\ninteractions, see Lou et al. [15]. These additional components enable the model to capture the\njoint effects of feature pairs on the response variable. Similarly to interactions in GLMs, GA \u00b2Ms\ntypically require manual identification and specification of interaction terms, which demands\ndomain expertise and time consuming feature engineering. This reliance on expert judgement\ncan introduce subjectivity and biases, and it may make the model sensitive to the modeler\u2019s\nchoices, potentially affecting both predictive performance and generalization capability.\nDeep neural networks (DNNs) have attracted a growing attention in actuarial science as power-\nful tools for predictive modeling; for an extended discussion see Richman [23, 24]. These models\ncan be viewed as an extension of GLMs that allow for the modeling of nonlinear relationships;\nsee Chapter 5 of W\u00a8 uthrich et al. [35]. In particular, based on a successful training algorithm,\nDNNs perform automated feature engineering. Through the mechanism of representation learn-\ning, DNNs transform original input features into new, higher-level representations via multiple\nlayers of nonlinear functions, which are optimized to predict the response variable. This hierar-\nchical structure allows the model to automatically construct features that capture higher-order\ninteractions among the original covariates. Despite their strong predictive performance, DNNs\npresent significant challenges, particularly in identifying, quantifying and interpreting feature\ninteractions. The complexity of the learned representations makes it difficult to understand\nhow input variables jointly contribute to the model\u2019s predictions. For these reasons, DNNs are\nsometimes called black box models.\nModern machine learning literature grapples with the challenge of effectively modeling and in-\nterpreting feature interactions within neural networks. Song et al. [27] address this issue by\nproposing a specialized DNN architecture designed to automatically learn feature interactions\nin presence of tabular data. The model employs a multi-head self-attention mechanism, in-\nspired by the Transformer architecture of Vaswani et al. [31], to explicitly capture interactions\nbetween features, thereby eliminating the need for manual feature engineering. Tsang et al.\n[30] introduce a method for detecting statistical interactions in trained DNNs by analyzing the\nvalues of the learned weights. Li et al. [14] proposes modeling categorical feature interactions by\nrepresenting covariates using a graph, where each node corresponds to a distinct feature field.\nInteractions among features are effectively captured through the propagation mechanisms of a\nGraph Neural Network (GNN). Enouen\u2013Liu [7] develop a DNNs-based approach that extends\nclassical GAMs by introducing an additional component to capture higher-order feature inter-\nactions. Havrylenko\u2013Heger [13] revisit the CANN proposal of W\u00a8 uthrich\u2013Merz [33] to identify\n2\n\n--- Page 3 ---\nthe most significant interacting pairs missing in a GLM.\nThe contribution of this paper is to introduce the Tree-like Pairwise Interaction Network (PIN),\na novel neural network architecture designed to model and identify pairwise feature interactions\nin tabular data. The PIN architecture embeds each input feature into a learned latent space\nand then explicitly models all pairwise interactions through a shared feed-forward neural net-\nwork. Sharing this feed-forward neural network implies an efficient way of dealing with model\nparameters, and to still allow for variability between the different interactions, each interaction\nmakes use of a dedicated set of learnable parameters \u2013 playing a similar role to the Classification\n(CLS) token used in Devlin et al. [5] \u2013 which is aimed at modeling the dependence structure\nbetween each pair of features. The output of each interaction is passed through a centered hard\nsigmoid activation function that mimics the discrete partitioning behavior of decision trees, yet\nin a continuous and differentiable form. This results in tree-like structures (using binary splits),\nand this model allows for direct inspection of interaction terms allowing for model explainabil-\nity. The proposed PIN also exhibits connections with several well-established machine learning\nmodels, including gradient boosting, GA \u00b2M, and graph neural networks. These relationships\nare explored in detail throughout the manuscript. We assess the performance of the Tree-like\nPIN on the widely studied French motor third-party liability claims frequency dataset. Our ex-\nperiments demonstrate that the PIN architecture delivers strong predictive accuracy in terms of\nPoisson losses, outperforming not only classical benchmarks but also more recent neural network\narchitectures, such as the Credibility Transformer introduced by Richman et al. [25]. Moreover,\nunlike most other network architectures, the PIN architecture allows for an efficient computation\nof SHapley\u2019s Additive exPlanations (SHAP). The crucial point is that the PIN architecture only\ninvolves pairwise interactions, and it has been shown in Lundberg [16] and Mayer\u2013W\u00a8 uthrich\n[20, Proposition 3.1] that this allows for a very efficient computation of the Shapley values using\npaired permutation SHAP sampling.\nConcluding this introduction: Our novel architecture of the tree-like PIN provides superior\nperformance on tabular data over currently used network architectures, and at the same time\nit provides explainability. We believe that this makes the tree-like PIN an interesting proposal\nfor actuarial pricing.\nThe rest of this paper is organized as follows. In Section 2, we introduce the PIN architec-\nture, starting with feature embeddings and proceeding through the PIN. Section 3 details the\napplication of the PIN to claims frequency modeling. In Section 4.1, we propose interaction\nimportance measures for model interpretation, and we illustrate how the SHAP explanation can\nbe computed within the PIN architecture using paired-sampling permutation SHAP. Finally,\nSection 5 concludes with a summary and future research directions.\n2 Tree-like pairwise interaction network\n2.1 Feature embedding\nWe start from tabular input data x= (x1, . . . , x q)\u22a4. In a first step, we tokenize this tabular\ninput data to an input tensor \u03d5=\u03d5(x) = [ \u03d51, . . . , \u03d5 q]\u2208Rd\u00d7q; this step is done in complete\nanalogy to Gorishniy et al. [11], Brauer [2] and Richman et al. [25]. For this first step, we select\na fixed embedding dimension d\u2208N; this is a hyper-parameter selected by the modeler.\n3\n\n--- Page 4 ---\nCategorical features. For categorical feature components xj\u2208 Xj={1, . . . , n j} \u2282N, having\nnjlevels, consider a d-dimensional entity embedding \u03d5j:Xj\u2192Rd\nxj7\u2192\u03d5j(xj) =njX\nk=1W(0)\nj,k1{xj=k}, (2.1)\nwhere W(0)\nj= [W(0)\nj,1, . . . , W(0)\nj,nj]\u22a4\u2208Rnj\u00d7dis an embedding matrix.\nContinuous features. For continuous feature components xj\u2208R, we consider a d-dimensional\nFNN embedding \u03d5j:R\u2192Rd\nxj7\u2192\u03d5j(xj) =W(2)\njtanh\u0010\nW(1)\njxj+b(1)\nj\u0011\n+b(2)\nj (2.2)\nwith biases b(1)\nj\u2208Rd\u2032andb(2)\nj\u2208Rd, weight matrices W(1)\nj\u2208Rd\u2032\u00d71andW(2)\nj\u2208Rd\u00d7d\u2032, for a fixed\nnumber of units d\u2032, and where the hyperbolic tangent function is applied element-wise.\nCollecting all embedded components ( \u03d5j)q\nj=1= (\u03d5j(xj))q\nj=1of the input xgives us the tensor\n\u03d5=\u03d5(x) = [\u03d51, . . . , \u03d5 q] = [\u03d51(x1), . . . , \u03d5 q(xq)]\u2208Rd\u00d7q. (2.3)\nThis tensor \u03d5involves the embedding weights W(0)\nj\u2208Rnj\u00d7dfrom the categorical features (of\ndimension njd), and the FNN biases and weights ( b(1)\nj, b(2)\nj, W(1)\nj, W(2)\nj) from the continuous\nfeature embeddings (of dimension 2 d\u2032+ (d\u2032+ 1)d). All these biases and weights are learned from\nthe available training data.\n2.2 Pairwise interaction layer\nThe tensor \u03d5=\u03d5(x), given in (2.3) and containing the tokens ( \u03d5j(xj))q\nj=1, is expressing the\nindividual feature components of x\u2208 X before they have been interacting with each other.\nIn the next step, we are going to let these tokens interact in a pairwise manner. This can\nbe interpreted in a similar way as the key-query interaction concept in the attention layer of\nVaswani et al. [31].\nWe recall the (centered version of the) hard sigmoid activation\n\u03c3hard:R\u2192[0,1], x 7\u2192\u03c3hard(x) = max\u0012\n0,min\u0012\n1,1 +x\n2\u0013\u0013\n. (2.4)\nThis is the centered version of the hard sigmoid activation; there is also a non-centered version\nwith (1 + x)/2 being replaced by xin (2.4). The centered version is directly comparable to the\nsigmoid activation function and the step function activation, see Figure 1.\nWe now let \u03d5jand\u03d5kinteract, j\u2264k. To support these interactions we introduce interaction\ntokens ej,k\u2208Rd0, for a given hyper-parameter d0\u2208N. Before interacting, these interaction\ntokens do not carry any information; this is similar to the CLS tokens as used in Devlin et al. [5]\nand Richman et al. [25]. We concatenate the three vectors to a row-vector ( \u03d5j, \u03d5k,ej,k)\u22a4\u2208\nR2d+d0,j\u2264k. The goal of the interaction tokens is to learn the interaction structure within the\ntensor \u03d5for predicting the response Y, in particular, different pairs ( \u03d5j, \u03d5k) may have a different\ninteraction behavior which is reflected in different interaction tokens ej,k,j\u2264k. Unlike the CLS\ntokens of Devlin et al. [5] and Richman et al. [25], we do not use these interaction tokens to strip\n4\n\n--- Page 5 ---\n\u22124 \u22122 0 2 40.0 0.2 0.4 0.6 0.8 1.0activation functions\nxsigma(x)hard sigmoid\nsigmoid\nstep functionFigure 1: Activation functions: (centered) hard sigmoid, sigmoid and step function.\nout information, but we add them to allow for the necessary modeling flexibility to be able to\naccount for differences in the different pairs.\nDefine the shared interaction network f\u03b8:R2d+d0\u2192Ras a composition of three FNN layers\nf\u03b8(\u03d5j, \u03d5k,ej,k) =W(3)ReLU\u0010\nW(2)ReLU\u0010\nW(1)(\u03d5j, \u03d5k,ej,k)\u22a4+b(1)\u0011\n+b(2)\u0011\n+b(3),(2.5)\nwith:\n\u2022the 1st FNN layer has d1units, rectified linear unit (ReLU) activation, network weights\nW(1)\u2208Rd1\u00d7(2d+d0)and bias b(1)\u2208Rd1;\n\u2022the 2nd FNN layer has d2units, ReLU activation, network weights W(2)\u2208Rd2\u00d7d1and\nbiasb(2)\u2208Rd2;\n\u2022the output layer has 1 unit, linear activation, network weights W(3)\u2208R1\u00d7d2and bias\nb(3)\u2208R;\nand\u03b8= (W(1), b(1), W(2), b(2), W(3), b(3)) collects all network parameters. This interaction net-\nwork f\u03b8is going to be shared by all pairwise interactions between the components of \u03d5, i.e.,\nthere is one single network parameter \u03b8that is shared by all ordered 1 \u2264j\u2264k\u2264qpairs ( \u03d5j, \u03d5k).\nThis can also be seen as a so-called time-distributed network as being used, e.g., in recurrent\nneural networks. To differentiate different forms of interactions of the different ordered pairs,\nj\u2264k, we let the interaction tokens ej,klearn the pairwise interaction structures.\nDefinition 2.1 (pairwise interaction layer) For feature tokens \u03d5j(\u00b7)and\u03d5k(\u00b7),1\u2264j\u2264k\u2264\nq, and interaction tokens ej,k, we define the pairwise interaction units by\nhj,k(x) =\u03c3hard\u0010\nf\u03b8(\u03d5j(xj), \u03d5k(xk),ej,k)\u0011\n\u2208[0,1]. (2.6)\n5\n\n--- Page 6 ---\nThe pairwise interaction layer for x\u2208 X is defined by the upper-right triangular matrix\n(hj,k(x))1\u2264j\u2264k\u2264q=\uf8eb\n\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8ec\uf8edh1,1(x)h1,2(x)\u00b7\u00b7\u00b7 h1,q\u22121(x) h1,q(x)\nn/ah2,2(x)\u00b7\u00b7\u00b7 h2,q\u22121(x) h2,q(x)\n...............\nn/a n /a\u00b7\u00b7\u00b7 hq\u22121,q\u22121(x)hq\u22121,q(x)\nn/a n /a\u00b7\u00b7\u00b7 n/a hq,q(x)\uf8f6\n\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f7\uf8f8, (2.7)\nwhere the lower-left part is undefined indicated by n/a.\nExample 2.2 We give a simple example of the pairwise interaction layer. We assume only\ncontinuous features, that is, x\u2208Rq. We select one-dimensional linear feature embeddings\n(tokenization) xj7\u2192\u03d5j(xj) =xjfor all 1 \u2264j\u2264q. This gives us the tensor \u03d5=\u03d5(x) =\nx\u2208R1\u00d7q. For the shared interaction network f\u03b8we select a simple additive function and the\ninteraction tokens are set off in this example. This gives the simplified version\nhadd\nj,k(x) =\u03c3hard(xj+xk) for j\u2264k. (2.8)\nThis is illustrated on the left-hand side of Figure 2 for j < k .\nadditive interation network\nfeature x_jfeature x_k\n\u22122 \u22121 0 1 2\u22122 \u22121 0 1 2\nadditive interation network\nfeature x_jfeature x_k\n\u22122 \u22121 0 1 2\u22122 \u22121 0 1 2\nFigure 2: Interaction layers: (lhs) \u03c3hard(xj+xk) and (rhs) \u03c3hard(xj/3 +xk) for j < k .\nThe right-hand side of Figure 2 shows a linear additive interaction network with the first com-\nponent xjbeing scaled with 1 /3. We note that these linear additive interaction networks are\nrather similar to binary decision tree partitions of the feature space. We allow for additional\nflexibility by letting the interaction network f\u03b8be non-linear and by letting the interaction token\nej,klearn different pairwise interaction structures. It is precisely this ability to learn to modify\nthe inputs that gives the pairwise interaction layers their power. \u25a0\n2.3 Pairwise interaction network\nWe are now ready to define the tree-like pairwise interaction network.\n6\n\n--- Page 7 ---\nDefinition 2.3 Choose an output activation function g. The pairwise interaction network\n(PIN) is defined by fPIN:X \u2192R\nx7\u2192fPIN(x) =g\uf8eb\n\uf8edX\n1\u2264j\u2264k\u2264qwj,khj,k(x) +b\uf8f6\n\uf8f8, (2.9)\nwith learnable (real-valued) output weights (wj,k)1\u2264j\u2264k\u2264qand bias b\u2208R.\nThere is structural similarity between a PIN and a classical GBM\nfGBM(x) =MX\nm=1LX\nl=1\u03b3m,l1Rm,l(x),\nwith M\u2208NGBM iterations considering binary split decision trees with Lleaves giving the\npartition ( Rm,l)L\nl=1of the feature space Xand assigning the leaf values ( \u03b3m,l)L\nl=1to this partition.\nWe start by discussing the structural correspondence between the two regression functions.\nGBM component PIN analogue\nIndicators 1Rm,l(x) Bounded activation \u03c3hard(f\u03b8(\u00b7))\nLeaf values \u03b3m,l Output weights wj,k\nTree structure Network f\u03b8with interaction tokens ej,k\nWe list some key differences:\n1.Smooth activation : The hard binary decisions 1Rm,l(x) are replaced by continuous func-\ntions involving the hard sigmoid function.\n2.Learned representations : There is an automatic feature transformations through embed-\ndings resulting in the tensor \u03d5, whereas GBMs split on the original feature values.\n3.Parameter sharing : Common network f\u03b8across all interactions, and the interaction tokens\nlearn the differences between the pairwise interactions ( \u03d5j, \u03d5k).\n4.Joint optimization : All components trained simultaneously, where as the GBM learns the\nparameters recursively in stage-wise adaptive way.\nThe other close connection of the PIN is to a GA \u00b2M. A GA \u00b2M is obtained from a GAM by\nadding interaction terms to the regression function\ng\u22121(\u00b5(X)) =b+qX\nj=1fj(Xj) +X\n1\u2264j<k\u2264qfj,k(Xj, Xk), (2.10)\nwhere fjandfj,kare selected from a given class of splines; see Lou et al. [15]. A classical\nGAM only considers the terms ( fj)q\nj=1, and the GA \u00b2M adds the pairwise interaction terms\n(fj,k)1\u2264j<k\u2264q. We note that this GA \u00b2M (2.10) has the same structure as our PIN (2.9), the\ndifference is that we replace the splines ( fj)q\nj=1and ( fj,k)1\u2264j<k\u2264qby pairwise interaction layers\n(hj,k)1\u2264j\u2264k\u2264qbeing based on FNNs.\n7\n\n--- Page 8 ---\n2.4 Connection to graph neural networks\nThe pairwise interaction structure of the PIN can be naturally interpreted through the lens of\ngraph neural networks (GNNs). We formalize this connection and demonstrate how our model\ncan be viewed as a special case of message passing on a complete graph with learned edge\nfeatures.\nDefinition 2.4 (feature graph) For input x\u2208 X, we define the corresponding feature graph\nGx= (V, E)as follows:\n\u2022Vertices V={1, . . . , q }correspond to feature components;\n\u2022Edges E={(j, k) : 1\u2264j\u2264k\u2264q}form a complete graph;\n\u2022Node features \u03d5j=\u03d5j(xj)\u2208Rdforj\u2208V;\n\u2022Edge features ej,k\u2208Rd0for(j, k)\u2208E.\nOur PIN can be reformulated as a message passing neural network with a single update step.\nDefinition 2.5 (message function) The message function M:Rd\u00d7Rd\u00d7Rd0\u2192Ris defined\nas\nM(\u03d5j, \u03d5k,ej,k) =\u03c3hard(f\u03b8(\u03d5j, \u03d5k,ej,k)),\nwhere f\u03b8is the shared interaction network (2.5).\nIt is obvious that the PIN (2.9) is equivalent to the GNN defined by\nfGNN(x) =g\uf8eb\n\uf8edX\n(j,k)\u2208Ewj,kM(\u03d5j, \u03d5k,ej,k)\uf8f6\n\uf8f8.\nOur architecture differs from traditional GNNs in the following ways:\n1.Edge-centric : While most GNNs focus on learning node representations, our model pri-\nmarily focuses on edge interactions and the interaction token ej,kto distinguish different\ninteraction patterns.\n2.Learned edge features : The interaction tokens ej,kare learned parameters rather than\nderived features.\n3.Single-pass architecture : Unlike typical GNNs that use multiple layers of message passing,\nour model uses a single pass with a more expressive message function.\n4.Bounded messages : The hard sigmoid activation ensures bounded message values, similar\nto the binary nature of GBM binary decision tree splits.\nHaving this connection to GNNs provides the following additional theoretical insights.\nProposition 2.6 (permutation invariance) For any permutation \u03c0of the feature indices,\nthe model output is invariant up to a reindexing of the output weights wj,k.\n8\n\n--- Page 9 ---\nThis proposition immediately follows by analyzing permutations of the components/vertices\nV={1, . . . , q }.\nProposition 2.7 (expressiveness) The PIN can express any message-passing GNN with bi-\nnary messages, single-layer architecture and complete graph structure.\nUsing GNN operations, the GNN perspective suggests several natural extensions to our base\nmodel.\nDefinition 2.8 (multi-layer PIN) We can define a K-layer model through iterative message\npassing, that is,\n\u03d5(k+1)\nv = UPDATE\uf8eb\n\uf8ed\u03d5(k)\nv,X\nu\u2208N(v)M(k)(\u03d5(k)\nu, \u03d5(k)\nv,eu,v)\uf8f6\n\uf8f8 (2.11)\nwhere \u03d5(0)\nv=\u03d5v(xv),v\u2208V,k= 0, . . . , K \u22121, and N(v)are the vertices adjacent to v. The\nUPDATE can be various functions such as the maximum or the mean of the two components.\nRemark 2.9 (attention mechanism) The model can be enhanced with an attention mecha-\nnism by replacing the fixed output weights wj,kwith learned functions of the node features:\nwj,k=\u03b1\u03d1(\u03d5j, \u03d5k),\nwhere \u03b1\u03d1is a (self-)attention layer with parameters \u03d1; see Vaswani et al. [31]. If we apply this\nattention weights only to pairs j < k and if we apply a thresholding, we obtain\nx7\u2192g\uf8eb\n\uf8edb+kX\nj=1wjhj,j(x) +X\n1\u2264j<k\u2264qReLU\u0010\n\u03b1\u03d1(\u03d5j, \u03d5k)\u2212\u03c4\u0011\nhj,k(x)\uf8f6\n\uf8f8, (2.12)\nfor an attention threshold \u03c4\u2208R. The thresholding in (2.12) gives a natural way of pairwise\ninteraction selection.\nThe connection to GNNs not only provides theoretical insights but also suggests practical ex-\ntensions and implementation strategies for the PIN architecture. Moreover, for efficient imple-\nmentations by existing GNN libraries are available that allow for batched processing (graph\nbatching for parallel processing), optimized GNN kernels on GPUs, and optional sparsification\nof the interaction graph, similar to (2.12).\n3 Claims frequency example\nWe present an example that we take in several steps to illustrate the functioning of the tree-\nlike PIN. We consider the popular French motor third-party liability (MTPL) claims frequency\ndataset available from Dutang et al. [6]. We apply the same data cleaning as described in\nW\u00a8 uthrich\u2013Merz [34];1for a detailed discussion of the French MTPL dataset we refer to Appendix\nB of the latter reference.\n1The cleaned data is available from https://aitools4actuaries.com/ .\n9\n\n--- Page 10 ---\nOur goal is to predict the MTPL claims frequencies Yi=Ni/viof the insurance policyholders\ni\u2208 {1, . . . , n }, where Niis the number of claims of policyholder i,vi>0 is the time exposure\n(in yearly units) of that policyholder, and xidescribes their features. We fit different tree-like\nPINs to this data by using the Poisson deviance loss as the objective function; the Poisson\ndeviance loss is the canonical strictly consistent loss function for mean estimation for claims\ncount problems in insurance; Gneiting\u2013Raftery [10] and W\u00a8 uthrich et al. [35]. For given learning\ndataL= (Yi,xi, vi)n\ni=1, we aim at minimizing the following (average) Poisson deviance (PD)\nloss to find the optimal weights for the selected PIN\nLPD=1\nnnX\ni=12vi\u0012\nfPIN(xi)\u2212Yi\u2212Yilog\u0012fPIN(xi)\nYi\u0013\u0013\n, (3.1)\nwhere fPIN(\u00b7) is the PIN given in (2.9), and we select the exponential output function g(\u00b7) =\nexp(\u00b7) which is the canonical link of the Poisson model.\nFor model training (minimizing the Poisson deviance loss LPDin the network weights and biases),\nwe use the Adam optimizer on batches of size 128, with an initial learning rate of 0.001. We\nthen apply a learning rate reduction on plateau (factor 0.9, patience 5), and early stopping is\nexercised based on a 10% validation set of the learning data L; the size of the learning data is\nn= 610 ,206. Moreover, we hold an independent test sample T= (Yj,xj, vj)m\nj=1of sample size\nm= 67,801, that is going to be used for out-of-sample model validation and model comparison.\nThe learning and test samples, LandT, are identical to Brauer [2] and Richman et al. [25],\nwhich makes all results directly comparable to that reference.\nThe case of a linear PIN, with two continuous features only. Similar to Example\n2.2, we start with a linear PIN, and we only consider the two continuous features \u2018driver age\u2019\n(DrivAge) and \u2018log-Density\u2019. Moreover, we only consider the interaction term h1,2(x) and we\ndrop the diagonal terms h1,1(x) and h2,2(x) by setting w1,1=w2,2= 0. This allows us to nicely\nillustrate the results. For this linear off-diagonal PIN, we select the (single, q= 2) pairwise\ninteraction\nhlinear\n1,2(x) =\u03c3hard\u0010\nW(1)(x1, x2,e1,2)\u22a4+b(1)\u0011\n.\nThis only considers the most inner operation of the shared interaction network (2.5), with\nembedding dimension d= 1, interaction token dimension d0= 1 and output dimension d1=\n1. This gives us a weight matrix W(1)\u2208R1\u00d73and a bias b(1)\u2208R. This is similar to the\nadditive interaction (2.8), but additionally allowing for an affine transformation. Note that in\nthis example of only one interaction pair, (\u2018DrivAge\u2019, \u2018log-Density\u2019), the interaction token e1,2\nwould not be necessary. By inserting this linear interaction network into the PIN in (2.9) gives\nus a first illustrative example.\nWe fit this linear off-diagonal PIN to the learning data Lusing the Poisson deviance loss (3.1),\nand the result is presented in Figure 3. The colored dots show the estimated frequencies on the\ninsurance portfolio (for the white parts there are no insurance policies available for these feature\ncombinations). We observe that this 1-dimensional ( d= 1) linear off-diagonal PIN suggests a\ndiagonal pattern. Related to decision trees, this corresponds to a feature space partition along\nthe diagonal (for driver age and log-Density scaled as in Figure 3). In the next example, we are\ngoing to refine this prediction by allowing for more general functional forms.\n10\n\n--- Page 11 ---\n0.02.55.07.510.0\n20 40 60 80\ndriver agelog\u2212density\n0.060.080.10predicted\nfrequencytree\u2212like PIN: linear caseFigure 3: Heatmap of predicted frequencies of the linear off-diagonal PIN only considering the\nfeatures \u2018DrivAge\u2019 and \u2018log-Density\u2019.\nTree-like PIN, with two continuous features only. In our next example, we consider\nthe non-linear tree-like PIN as introduced in (2.9). To improve the modeling step-by-step on\nthe previous example, we again only consider the two features \u2018DrivAge\u2019 and \u2018log-Density\u2019 and\nwe let them interact non-linearly, and again we only consider the off-diagonal interaction term\nh1,2(x) by setting w1,1=w2,2= 0. We start with the 1-dimensional embedding case d= 1,\nthus, we consider 1-dimensional FNN embeddings xj7\u2192\u03d5j(xj)\u2208R, which we then let interact\nby the interaction network as defined in (2.5); note that because we only have two features\nj\u2208 {1, q= 2}in this example, this network is not shared by different interactions and the\ninteraction token is not necessary.\n0.02.55.07.510.0\n20 40 60 80\ndriver agelog\u2212density\n0.10.20.3predicted\nfrequencytree\u2212like PIN: two features and 1D embedding\n0.02.55.07.510.0\n20 40 60 80\ndriver agelog\u2212density\n0.10.20.3predicted\nfrequencyGLM: two features\nFigure 4: Heatmaps of predicted frequencies of (lhs) the off-diagonal PIN with 1D embedding\n(d= 1), and (rhs) GLM only considering the two features \u2018DrivAge\u2019 and \u2018log-Density\u2019.\nWe fit this (non-linear) off-diagonal PIN as described above by using the Adam optimizer and\n11\n\n--- Page 12 ---\nearly stopping. The result of this 1-dimensional case with two feature components is shown\nin Figure 4 (lhs), and it is compared to a GLM considering the same two features \u2018DrivAge\u2019\nand \u2018log-Density\u2019, the first variable is encoded categorically and the second one continuously in\nthis GLM, see W\u00a8 uthrich\u2013Merz [34, Section 5.2.4] for more details. The GLM in the right-hand\nside of Figure 4 shows a multiplicative structure in the two feature components (because of the\nlog-link choice), and the PIN on the left-hand side gives a refinement indicating that the vertical\nage class coloring of the GLM is not fully appropriate. From these two plots we also observe\nthat the expected frequency is non-monotone in the age of driver variable.\n0.02.55.07.510.0\n20 40 60 80\ndriver agelog\u2212density\n0.10.20.3predicted\nfrequencytree\u2212like PIN: two features and 4D embedding\n0.02.55.07.510.0\n20 40 60 80\ndriver agelog\u2212density\n0.10.20.3predicted\nfrequencyGLM: two features\nFigure 5: Heatmaps of predicted frequencies of (lhs) the off-diagonal PIN with 4D embedding\n(d= 4), and (rhs) GLM only considering the two features \u2018DrivAge\u2019 and \u2018log-Density\u2019.\nIn a next step, a further refinement is received by increasing the dimension of the embedding\ntod= 4 in xj7\u2192\u03d5j(xj)\u2208Rd. Similarly to GBMs, this overlaps multiple tree-like structures.\nFigure 5 (lhs) shows the resulting fitted off-diagonal PIN, which looks like a refinement of the\nGLM on the right-hand side, also more clearly reflecting the expected frequency increase in the\ndriver age bucket between 40 and 50. Note that this age bucket has not been indicated to the\nPIN, but this finding of a higher frequency in this age bucket is purely data driven. Out-of-\nsample on the test data T, the PIN has a slightly better performance than the GLM w.r.t. the\nPoisson deviance loss; we do not show the figures because in this example we only showcased\nthe functioning of the tree-like PIN on two feature components and only in the off-diagonal case\n(w1,1=w2,2= 0); the general case is presented next.\nTree-like PIN on all feature components. We now include all available feature compo-\nnents and we also consider the diagonal elements of the pairwise interaction layer (2.7). There\nare seven continuous and binary input variables to which we apply separate network embeddings\n(2.2), and two categorical variables to which we apply two entity embeddings (2.1). We select a\nhidden layer of dimension d\u2032= 20 in (2.2), and an entity embedding dimension of d= 10. This\ngives us a tensor \u03d5=\u03d5(x)\u2208R10\u00d79, and in total 10 \u00b79/2 = 45 interaction tokens ej,k\u2208R10,\n1\u2264j\u2264k\u22649; we select dimension d0= 10 for the interaction tokens. Finally, for the shared\ninteraction network (2.5), we select hidden layer dimensions d1= 30 and d2= 20. Table 1\n12\n\n--- Page 13 ---\nsummarizes the selected PIN architecture, indicating the number of weights/parameters to be\nfitted.\nModule # Weights\n\u2022Embedding dimension d= 10:\nContinuous features (7) with d\u2032= 20 1,750\nCategorical features (2) with nj= 11,22 330\n\u2022Pairwise interaction layer:\nInteraction tokens (10 \u00b79/2) with d0= 10 450\n1st FNN layer with d1= 30 930\n2nd FNN layer with d2= 20 620\nOutput layer 21\n\u2022Output:\nOutput weights ( wj,k)j\u2264kincluding bias 46\nTotal 4,147\nTable 1: Selected PIN architecture used on the French MTPL dataset.\nThis fully specifies the selected PIN architecture, which is now trained on the learning sample L\nusing the Poisson deviance loss (3.1) as the objective function. Because early stopped stochastic\ngradient descent (SGD) training involves several elements of randomness, the presented results\nare averages over 10 SGD runs using different seeds (starting points) for the SGD algorithm, see\nalso W\u00a8 uthrich et al. [35, Section 5.3.8]. That is, the loss figures are averages over 10 different fits\n(and in brackets we provide the observed standard deviations across these 10 runs). Moreover, we\nensemble the predictors (by averaging over the predictors). This ensembling typically improves\nthe predictive model, which is also verified in our PIN architecture, all these results are presented\nin Table 2.\n# In-sample Out-of-sample\nModel Param. Poisson loss Poisson loss\nNull model (intercept-only) [34] 1 25.213 25.445\nPoisson GLM3 [34] 50 24.084 24.102\nPoisson GAM (66.7) 23.920 23.956\nPlain-vanilla FNN [34] 792 23.728 ( \u00b10.026) 23.819 ( \u00b10.017)\nEnsemble plain-vanilla FNN [34] 792 23.691 23.783\nCAFFT [2] 27,133 23.715 ( \u00b10.047) 23.807 ( \u00b10.017)\nEnsemble CAFFT [2] 27,133 23.630 23.726\nCredibility Transformer [25] 1,746 23.641 ( \u00b10.053) 23.788 ( \u00b10.040)\nEnsemble Credibility Transformer [25] 1,746 23.562 23.711\nTree-like PIN 4,147 23.593 ( \u00b10.046) 23.740 ( \u00b10.025)\nEnsemble Tree-like PIN 4,147 23.522 23.667\nTable 2: Number of parameters, in-sample and out-of-sample Poisson deviance losses (units are\nin 10\u22122); benchmark models are taken from [34, Table 7.9], [2, Tables 2 and 4] and [25, Table\n2].\nLet us discuss the results of Table 2. The benchmark models are taken from [34, Table 7.9],\n[2, Tables 2 and 4] and [25, Table 2]. The null model does not consider any features, but it\n13\n\n--- Page 14 ---\nis just based on the empirical mean. Poisson GLM3 is a feature-engineered GLM having 50\nparameters; this model is linear on the canonical scale and does not consider any interaction\nterms. The linearity of the GLM is challenged by the GAM using natural cubic splines. The\ncomplexity of this GAM cannot be expressed in terms of the number of parameters involved,\nbut the GAM routine in Rdetermines the effective degrees of freedom (edf) using generalized\ncross-validation (GCV). The resulting edf in our example are 66.7, which means that suitably\nchosen natural cubic splines should be able to accommodate with roughly 67 parameters. These\nthree fitted models have been fitted by minimizing the Poisson deviance loss which is equivalent\nto perform MLE in the corresponding Poisson claims count model. We conclude that the GAM\nhas the best out-of-sample performance among these three classical actuarial models, resulting\nin an out-of-sample loss of 23 .956\u00b710\u22122. This is the best benchmark that we can get if we do\nnot consider any interaction terms between the feature components.\nIf we take a deep (plain-vanilla) FNN architecture of depth 3 with (20 ,15,10) units and with\ntwo-dimensional embedding layers for the two categorical covariates, we receive a network archi-\ntecture with 792 network parameters. If we fit this architecture with an early stopped SGD with\n10 initializations, we receive an average out-of-sample loss of 23 .819\u00b710\u22122, and ensembling these\n10 networks gives an out-of-sample performance of 23 .783\u00b710\u22122, thus, interaction modeling can\nclearly improve the GAM.\nThis deep plain-vanilla FNN architecture has been improved by using different versions of the\ntransformer architecture of Vaswani et al. [31] modified to tabular input data. The powerful\nadoptions of Brauer [2, Table 4] and Richman et al. [25, Table 2] provide out-of-sample losses of\n23.726\u00b710\u22122and 23 .711\u00b710\u22122, respectively, for the ensemble versions. From Table 2 we conclude\nthat our tree-like PIN proposal outperforms all these other methods. Thus, this data example\nverifies the superior predictive performance of the tree-like PIN over some of the most recent\nproposals in the literature. On top, as will be illustrated in the remainder of this article, it also\ngives us an intrinsic explainability, in contrast to most other network architectures.\n0.02.55.07.510.0\n20 40 60 80\ndriver agelog\u2212density\n0.10.20.30.40.5predicted\nfrequencytree\u2212like PIN\n0.02.55.07.510.0\n20 40 60 80\ndriver agelog\u2212density\n0.10.20.30.40.5predicted\nfrequencyGLM\nFigure 6: Heatmaps of predicted frequencies of (lhs) tree-like PIN and (rhs) GLM on all available\nfeatures.\nFigure 6 shows the resulting heatmaps of the predicted frequencies, with driver\u2019s age on the\n14\n\n--- Page 15 ---\nx-axis and log-Density on the y-axis; this is a two-dimensional Marginal plot (M plot); see\nApley\u2013Zhu [1] and W\u00a8 uthrich et al. [35, Section 9.3.4]. Remark that we do not expect a smooth\ngraph, because the predicted frequencies were computed on nine feature components, and the\ngraphs show a projected version to two of them (M plot), i.e., these are marginalized plots.\nE.g., for high driver\u2019s age we see some sprinkling of red dots which can only be explained by\nanalyzing the other features, e.g., it may be severely impacted by a high bonus-malus level;\nremark also that the portfolio is comparably sparse for large driver\u2019s age values, that is why\nextreme expected frequency values do not average out.\n4 Model explainability\n4.1 PIN interaction importance\nA notoriously difficult problem in machine learning models is explainability. Scholars have\ndeveloped various post-hoc measures of variable importance, e.g., the Permutation Variable\nImportance (VPI) of Breiman [3] and Fisher et al. [8], or SHAP importance of Lundberg\u2013Lee\n[17] and Mayer\u2013Stando [19] are two popular examples. When it comes to interactions, things\nbecome even more difficult. There is Friedman\u2013Popescu [9]\u2019s H2-statistics which is a post-hoc\nexplainability tool that measures the impact of bivariate components on the regression function\n(by partial dependence functions). Apart from that there is little guidance on measuring the\ninteraction strengths at a reasonable computational effort, i.e., testing pairwise interactions can\nbe very costly.\nComing back to the PIN (2.9), we modify this function in a way so that we do not aim at\ngetting the maximal predictive power, but we try to evaluate which shared interaction unit\nhj,k(x) contributes most to a loss decrease. For this, we consider the multi-output mapping\nx7\u2192g \nb+qX\nl=1wl,lhl,l(x) +bj,k+wj,khj,k(x)!\n1\u2264j<k\u2264q\u2208Rq(q\u22121)/2, (4.1)\nfor biases bj,k\u2208Rand the remaining terms have the same structure as in (2.9). The first part\nof this multi-output regression function is identical for all components 1 \u2264j < k\u2264q, that is,\nb+qX\nl=1fl(Xl) =b+qX\nl=1wl,lhl,l(x), (4.2)\nwhere we make the link to the GAM (2.10). The remaining terms hj,k(x) in (4.1), j < k ,\ncorrespond to the off-diagonal terms of the pairwise interaction layer (2.7). Each component in\n(4.1) adds another interaction term to the diagonal (non-interacting) terms (4.2), and we can\ndetermine the term hj,k(x),j < k , that leads to the biggest decrease in out-of-sample loss when\nadded to (4.2). The great thing is that we can fit all q(q\u22121)/2 interaction terms in the same\ngradient descent run, i.e., we apply the objective function for model fitting (simultaneously) to\nall components of (4.1), and jointly fit all components by just aggregating these component-\nwise losses. This is possible here because we have isolated all interaction pairs in the pairwise\ninteraction layer (2.7).\nIn our example, we consider one further simplification, namely, we replace the diagonal (non-\ninteracting) terms (4.2) by the fitted GAM terms that are considered to be optimal under the\n15\n\n--- Page 16 ---\nVehBrand:RegionVehAge:RegionDrivAge:VehBrandVehAge:DrivAgeVehAge:VehBrandDrivAge:RegionBonusMalus:RegionBonusMalus:VehBrandVehAge:BonusMalusDrivAge:BonusMalus\n0.000 0.005 0.010\nPairwise H (unnormalized)pairwise H^2\u2212statistics\nVehBrand:RegionVehAge:VehBrandBonusMalus:VehGasVehPower:BonusMalusArea:BonusMalusBonusMalus:DensityVehAge:BonusMalusDrivAge:BonusMalusBonusMalus:RegionBonusMalus:VehBrand\n0.000 0.025 0.050 0.075 0.100\ndecrease in lossPIN interaction importance \u2212 1Figure 7: (lhs) Friedman\u2013Popescu [9]\u2019s H2-statistics, and (rhs) PIN interaction importance.\nexclusion of interactions, see Table 2. This fitted GAM is kept fixed (frozen) and only the\noff-diagonal interaction terms in (4.1) are trained with SGD. We then compute the decrease in\nout-of-sample loss compared to the GAM for each individual interaction term. Figure 7 (rhs)\nshows the results (ranked by size). The biggest decrease in loss compared to the GAM can\nbe achieved by adding an interaction term for (BonusMalus:VehBrand), and the second biggest\ndecrease is obtained by adding (BonusMalus:Region).\nFigure 7 (lhs) illustrates Friedman\u2013Popescu [9]\u2019s H2-statistics of a fitted ML regression model.\nNot surprisingly, the same feature interactions seem to be the most impactful ones, but the\norder of the most important interactions is different. Friedman\u2013Popescu\u2019s H2-statistics studies\nthe partial dependence function in a fully fitted model (which can be seen as a top-down view),\nwhereas our PIN approach considers which interaction should be added to have the biggest\nimpact on the prediction accuracy (which is more of a bottom-up consideration). Thus, we can\ninterpret this as a forward selection of interaction terms, similar to the forward selection model\nbuilding in GLMs. Moreover, Friedman\u2013Popescu\u2019s H2-statistics is based on Partial Dependence\nPlots (PDP) which have the weakness that they are not reliable in cases where the feature\ncomponents are highly correlated. Not surprisingly, this weakness also translates to the H2-\nstatistics. Our method does not suffer form this weakness.\nSimilar to the method of forward selection in GLMs, we add the most significant interaction term\n(BonusMalus:VehBrand) to the diagonal regression model (4.2). We also freeze this interaction\nterm, and we repeat the same fitting strategy to find the second most important interaction\nterm, after (BonusMalus:VehBrand) has been added to the model. The result of this second\nround forward selection is shown in Figure 8 (rhs), and it is compared to the first round results\non the left-hand side. The second interaction that should be added is (DrivAge:BonusMalus).\nThis gives a systematic way of model building that may also be useful in improving GLMs and\nGAMs; this is similar to the forward selection algorithm of GLM model building. Note that\nthis forward selected model should be refitted simultaneously on all included terms to obtain\nthe optimal model, because in this forward inclusion process we always freeze the previously\n16\n\n--- Page 17 ---\nVehBrand:RegionVehAge:VehBrandBonusMalus:VehGasVehPower:BonusMalusArea:BonusMalusBonusMalus:DensityVehAge:BonusMalusDrivAge:BonusMalusBonusMalus:RegionBonusMalus:VehBrand\n0.000 0.025 0.050 0.075 0.100\ndecrease in lossPIN interaction importance \u2212 1\nVehAge:VehBrandDensity:RegionVehPower:DrivAgeVehAge:RegionBonusMalus:DensityVehBrand:RegionArea:BonusMalusVehAge:BonusMalusBonusMalus:RegionDrivAge:BonusMalus\n0.00 0.01 0.02 0.03\ndecrease in lossPIN interaction importance \u2212 2Figure 8: (lhs) PIN interaction importance - 1st round (this plot is identical to Figure 7, rhs),\nand (rhs) PIN interaction importance - 2nd round.\nselected terms.\n4.2 SHapley\u2019s Additive exPlanation\nThere is another very remarkable property of the Tree-like PIN architecture, namely, it allows\nfor very an efficient computation of SHapley\u2019s Additive exPlanation (SHAP) as introduced by\nLundberg\u2013Lee [17]. Shapley values originate from cooperative game theory. They are used to\nshare a total payoff of a cooperative game in a fair way among qplayers; see Shapley [26].\nDenote by C \u2286 Q ={1, . . . , q }any coalition of qplayers. Based on Shapley\u2019s fairness axioms,\nthere is precisely one way to allocate the total payoff to the qplayers of the cooperative game,\nresulting in the Shapley values \u03c8jassigned to players j\u2208 Q, given by\n\u03c8j=1\nqX\nC\u2286Q\\{ j}\u0012q\u22121\n|C|\u0013\u22121\u0010\n\u03bd(C \u222a { j})\u2212\u03bd(C)\u0011\n, for all j\u2208 Q, (4.3)\nfor a given value function C 7\u2192 \u03bd(C). In fact, starting from the given value function \u03bd, the\nShapley values ( \u03c8j)q\nj=1are the unique solution to a set of four axioms called Shapley\u2019s fairness\naxioms; see Shapley [26] and Lundberg\u2013Lee [17].\nThere are two equivalent reformulations of the Shapley values (4.3), namely, there is the kernel\nSHAP formulation of Lundberg\u2013Lee [17, Theorem 2] and there is the permutation SHAP version\nof\u02c7Strumbelj\u2013Kononenko [28, 29]. In this work we focus on the permutation SHAP formulation.\nDenote by \u03c0= (\u03c01, . . . , \u03c0 q) a permutation of the ordered set (1 , . . . , q ). Let \u03ba(j)\u2208 Q be the\nindex defined by \u03c0\u03ba(j)=j, and set\nC\u03c0,j=\b\n\u03c01, . . . , \u03c0 \u03ba(j)\u22121\t\n\u2282 Q . (4.4)\nThese are all components of \u03c0preceding \u03c0\u03ba(j)=j; for \u03ba(j) = 1, this is the empty set \u2205. The\npermutation SHAP version of \u02c7Strumbelj\u2013Kononenko [28, 29] proves that the Shapley values\n17\n\n--- Page 18 ---\n(4.3) can equivalently be computed by\n\u03c8j=1\nq!X\n\u03c0\u03bd(C\u03c0,j\u222a {j})\u2212\u03bd(C\u03c0,j), (4.5)\nwhere the summation runs over all permutations \u03c0. These are q! permutations \u03c0, which can\nbe an extremely large number making Shapley values computationally unattainable for large q.\nHowever, there is a nice result which says that if the value function \u03bdonly includes interactions\nof maximal order two, then an arbitrary permutation \u03c0= (\u03c01, . . . , \u03c0 q) and its reverted version\n\u03c1(\u03c0) = (\u03c0q, . . . , \u03c0 1) is sufficient to compute the Shapley values\n\u03c8j=1\n2\u0010\n\u03bd(C\u03c0,j\u222a {j})\u2212\u03bd(C\u03c0,j) +\u03bd\u0000\nC\u03c1(\u03c0),j\u222a {j}\u0001\n\u2212\u03bd\u0000\nC\u03c1(\u03c0),j\u0001\u0011\n, (4.6)\nif the value function \u03bdis a bilinear form; see Mayer\u2013W\u00a8 uthrich [20, Proposition 3.1]. For example,\nthe permutation \u03c0= (1, . . . , q ) and its reverted version \u03c1(\u03c0) = (q, . . . , 1) do the job.\nFor explaining machine learning predictions, one typically works on the link scale, i.e., one drops\nthe link function gin (2.9). For a given feature x, this motivates the total value of the grand\ncoalition Q\n\u03bd(Q) :=\u03bdx(Q) :=g\u22121(fPIN(x)) =X\n1\u2264j\u2264k\u2264qwj,khj,k(x) +b.\nThis is the prediction made by x= (x1, . . . , x q)\u22a4, and we want to explain how the feature\ncomponents xjcontribute to this prediction. This requires the choice of the value function\n\u03bd(C) =\u03bdx(C) for any coalition C \u2282 Q if the elements in Q \\ C are masked (not available for\nprediction). The most popular choice for describing the masking is the so-called interventional\nSHAP version, in our case given by\n\u03bd(C) :=\u03bdx(C) :=E\uf8ee\n\uf8f0X\n1\u2264j\u2264k\u2264qwj,khj,k\u0000\nxC,XQ\\C\u0001\n+b\uf8f9\n\uf8fb, (4.7)\nwhere the components xC= (xj)j\u2208Care kept fixed and the components XQ\\C= (Xj)j\u2208Q\\C\nare resampled from the population distribution X\u223cP. An important point in the subsequent\ncomputation of the Shapley values is that each term hj,kin (4.7) only depends on the components\n(j, k), i.e., we only have pairwise interactions, see (2.6). That is, by a slight abuse of notation,\nhj,k\u0000\nxC,XQ\\C\u0001\n=\uf8f1\n\uf8f4\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f3hj,k(xj, xk) if j, k\u2208 C,\nhj,k(Xj, xk) if j\u2208 Q \\ C andk\u2208 C,\nhj,k(xj, Xk) if j\u2208 Candk\u2208 Q \\ C ,\nhj,k(Xj, Xk) if j, k\u2208 Q \\ C .\nThis interventional SHAP formula (4.7) is usually estimated empirically. For this we select a\nbackground set Bfrom the learning sample L. This allows one to determine the value function\nempirically by\nb\u03bd(C) =b\u03bdx(C) =1\n|B|X\ni\u2208B\uf8ee\n\uf8f0X\n1\u2264j\u2264k\u2264qwj,khj,k\u0010\nxC,X(i)\nQ\\C\u0011\n+b\uf8f9\n\uf8fb, (4.8)\nwhere X(i)is the i-th instance of the background dataset.\n18\n\n--- Page 19 ---\nThe crucial point now is that the value function C 7\u2192b\u03bd(C) =b\u03bdx(C) only involves interactions\nof maximal order two. Therefore, the paired permutation SHAP formula (4.6) applies (and is\nexact for the selected value function b\u03bd=b\u03bdx). This requires 2( q+ 1) evaluations of the value\nfunction b\u03bd, and each value function evaluation requires |B|computations of PIN predictions,\nsee (4.8). Thus, altogether the SHAP decomposition of the prediction g\u22121(fPIN(x)) requires\n2(q+ 1)|B|PIN network computations for computing the exact Shapley values ( \u03c8j)q\nj=1of the\nvalue function b\u03bd. Any other network architecture that involves higher order interactions would\nneed q!|B|network computations for receiving the exact Shapley values of the corresponding\nestimated interventional SHAP value function, thus, the order of computations grows as q!\ninstead of 2( q+ 1) as in our Tree-like PIN case.\nWe revisit the PIN architecture depicted in Table 1 having the best predictive performance of\nall models presented in Table 2. We emphasize that this network architecture only considers\ninteractions of order two on the link scale, thus, the paired-sampling permutation SHAP formula\n(4.6) applies, and it gives exact results, for instance, we can use the permutation pair \u03c0=\n(1, . . . , q ) and \u03c1(\u03c0) = (q, . . . , 1). As background dataset we select at random |B|= 2000 instances\nfrom the learning set. This allows us to decompose the predictions g\u22121(fPIN(x)) of selected\ninstances x. Decomposing 100 predictions takes 23 seconds on an ordinary laptop.2\n\u22120.050.030.030.01\n\u22120.38\n0.07\n\u22120.13\n0\n\u22120.05\u22120.4\u22120.3\u22120.2\u22120.10.0\nArea\nVehPower\nVehAge\nDrivAge\nBonusMalus\nVehGas\nDensity\nVehBrand\nRegion\nfeature componentsShapley valueswaterfall plot of a single instance\nAreaVehPowerVehAgeRegionVehBrandVehGasDensityDrivAgeBonusMalusSHAP variable importance\n0.0 0.1 0.2 0.3 0.4\nFigure 9: (lhs) SHAP waterfall graph ( \u03c8j)q\nj=1of a single instance x, and (rhs) SHAP variable\nimportance.\nThe left-hand side of Figure 9 shows the Shapley values ( \u03c8j)q\nj=1of a single instance x. We have\nglobal mean of the background dataset of b\u03bdx(\u2205) =\u22122,776, and the prediction of feature xis\ngiven by b\u03bdx(Q) =g\u22121(fPIN(x)) =\u22123.239. Thus, feature xhas a prediction being \u22120.463 below\naverage. The Shapley values ( \u03c8j)q\nj=1illustrated in Figure 9 (lhs) show how this lower prediction\ncan be explained. The biggest decrease is explained by the bonus-malus level and the density\nfeatures, whereas the vehicle gas feature leads to the biggest increase in the explanation of this\ndifference from b\u03bdx(\u2205) tob\u03bdx(Q).\n2Intel Core i7-1355U Processor (12M Cache, up to 5.00 GHz)\n19\n\n--- Page 20 ---\nGoing from local to global SHAP explanations, we can decompose the predictions g\u22121(fPIN(xi))\nof many instances ( xi)i\u2208I, for a randomly selected subset Iof the entire data. This provides us\nwith SHAP decompositions ( \u03c8(i)\nj)q\nj=1which allow us to consider different statistics. The following\nanalysis is based on |I|= 1000 instances xithat have been decomposed by the paired-sampling\npermutation SHAP method. SHAP variable importance is then obtained by considering\n\u03c8j=1\n|I|X\ni\u2208I|\u03c8(i)\nj|.\nFigure 9 (rhs) shows these importance measures \u03c8jfor all feature components j\u2208 Q. The most\nsignificant role is played by the bonus-malus level (past claims history) followed by the driver\u2019s\nage variable, at the other end is the area code which seems to be the least important variable.\n50 60 70 80 90100 110 120boxplot BonusMalus vs. DrivAge\nDrivAgeBonusMalus\n18\n19\n20\n21\n22\n23\n24\n25\n26\u221235\n36\u221245\n46\u221255\n56\u221265\n66\u221275\n76+\n2030405060708090\u22120.4 \u22120.2 0.0 0.2Shapley values DrivAge\nDrivAgeShapley valuesBonusMalus\n50\n60\n70\n80\n90\n100\n110\n120\n130\n140\n150\nFigure 10: (lhs) Dependence between driver\u2019s age and the bonus-malus variable, this graph is\ntaken from [34, Figure 13.12], and (rhs) Shapley values \u03c8(i)\njof the selected 100 instances i\u2208 I\nfor the covariate j=DrivAge ; the coloring is the bonus-malus level of these instances.\nIn Figure 10 (rhs), we analyze the 1000 decomposed instances xi,i\u2208 I. The plot shows\nthe Shapley values \u03c8(i)\njof these 1000 instances for the variable j=DrivAge . This driver age\nvariable has its strongest interaction with the bonus-malus level, see Figure 8, and the coloring\nof the Shapley values in Figure 10 (rhs) is selected according to these bonus-malus levels. We\nobserve that the driver\u2019s age variable has a decreasing effect on the prediction for drivers\u2019 ages\nbelow 40 (negative Shapley values) and an increasing effect for ages between 40 and 60 (positive\nShapley values). To fully understand this picture, one needs to understand the interaction with\nthe bonus-malus level, and we observe for drivers\u2019 ages below 40, that this positive age effect\nespecially holds for low bonus-malus levels. In fact, there is a strong dependence between the\ndriver\u2019s age variable and the bonus-malus level for drivers\u2019 ages below 40, see boxplot of Figure\n10 (lhs), because new drivers start at a bonus-malus level of 100 (yellow color in Figure 10, rhs).\nThis level decreases in every year that the driver is free of accidents, and otherwise it increases.\nTherefore, young drivers typically have a higher bonus-malus level than mature drivers (in a low\nfrequency environment), and the Shapley values in Figure 10 (rhs) tell us that young drivers\n20\n\n--- Page 21 ---\nwith a lower bonus-malus level (green colors) get a decrease compared to their older peers in\nthe same bonus-malus class. Of course, this interaction makes perfect sense. This completes our\nexample.\n5 Conclusion\nThis paper introduces the Tree-like Pairwise Interaction Network (PIN), a novel neural net-\nwork architecture designed to explicitly capture pairwise covariate interactions in tabular data.\nModeling such interactions is essential in predictive tasks, as they reveal how covariates jointly\naffect the response beyond their individual contributions. The Tree-like PIN embeds each input\ncovariate into a latent space, and it models pairwise interactions through a shared feed-forward\nneural network equipped with a centered hard sigmoid activation, which replicates the parti-\ntioning behavior of decision trees. The use of a shared network ensures parameter efficiency,\nwhile the inclusion of interaction-specific learnable parameters preserves the flexibility required\nto capture heterogeneous dependencies across covariate pairs.\nNumerical experiments show that the Tree-like PIN achieves a strong predictive performance\nand it consistently outperforms both traditional benchmarks and recently introduced neural\nnetwork architectures, including the Credibility Transformer of Richman et al. [25]. In addi-\ntion, PIN enhances model interpretability by enabling the analysis of how pairs of covariates\njointly contribute to the response. Since this architecture only involves pairwise interactions,\nit allows for a very efficient way of computing SHapley\u2019s Additive exPlanations (SHAP) by the\npaired-sampling permutation SHAP method. The paper also highlights connections between\nthe functioning of the PIN architecture and existing methods. Despite its novel architecture,\nPIN shares similarities with GA \u00b2Ms, gradient boosting, and graph neural networks.\nPossible future research directions include the incorporation of regularization techniques to pro-\nmote sparsity, thereby enabling an automated selection of the most relevant interaction pairs\nand improving model generalization. Such regularization strategies could also be employed to\npenalize terms involving protected attributes, supporting the development of fairer predictive\nmodels. Another promising avenue is the extension of the PIN architecture to time series data,\nenabling the joint modeling of temporal dependencies and feature interactions.\nConflict of interest. The authors are not aware of any conflict of interest.\nCode and data availability. Code and data is available from the GitHub repository:\nhttps://github.com/wueth/Tree-Like-PIN\nReferences\n[1] Apley, D.W., Zhu, J. (2020). Visualizing the effects of predictor variables in black box\nsupervised learning models. Journal of the Royal Statistical Society, Series B: Statistical\nMethodology 82/4 , 1059-1086.\n[2] Brauer, A. (2024). Enhancing actuarial non-life pricing models via Transformers. European\nActuarial Journal 14/3 991-1012.\n[3] Breiman, L. (2001). Random forests. Machine Learning 45/1 , 5-32.\n21\n\n--- Page 22 ---\n[4] Chang, L., Gao, G., Shi, Y. (2024). Claims reserving with a robust generalized additive\nmodel. North American Actuarial Journal 28/4 , 840\u2013860.\n[5] Devlin, J., Chang, M.-W., Lee, K., Toutanova, K. (2018). BERT: Pre-training of deep\nbidirectional Transformers for language understanding. arXiv: 1810.04805.\n[6] Dutang, C., Charpentier, A., Gallic, E. (2024). Insurance dataset. Recherche Data Gouv .\nhttps://github.com/dutangc/CASdatasets\n[7] Enouen, J., Liu, Y. (2022). Sparse Interaction Additive Networks via Feature Interac-\ntion Detection and Sparse Selection. Advances in Neural Information Processing Systems\n(NeurIPS) 35, 23410\u201323424.\n[8] Fisher, A., Rudin, C., Dominici, F. (2018). All models are wrong, but many are useful:\nLearning a variable\u2019s importance by studying an entire class of prediction models simulta-\nneously. arXiv :1801.01489\n[9] Friedman, J.H., Popescu, B.E. (2008). Predictive learning via rule ensembles. The Annals\nof Applied Statistics 2/3, 916-954.\n[10] Gneiting, T., Raftery, A.E. (2007). Strictly proper scoring rules, prediction, and estimation.\nJournal of the American Statistical Association 102/477 , 359-378.\n[11] Gorishniy, Y., Rubachev, I., Khrulkov, V., Babenko, A. (2021). Revisiting deep learning\nmodels for tabular data. In: Beygelzimer, A., Dauphin, Y., Liang, P., Wortman Vaughan,\nJ. (eds). Advances in Neural Information Processing Systems ,34. Curran Associates, Inc.,\nNew York, 18932-18943.\n[12] Hastie, T., Tibshirani, R. (1986). Generalized additive models (with discussion). Statistical\nScience 1, 297\u2013318.\n[13] Havrylenko, Y., Heger, J. (2024) Detection of interacting variables for generalized linear\nmodels via neural networks. European Actuarial Journal 14/2 , 551-580.\n[14] Li, Z., Cui, Z., Wu, S., Zhang, X., Wang, L. (2019). Fi-GNN: Modeling Feature Interactions\nvia Graph Neural Networks for CTR Prediction. In Proceedings of the 28th ACM Interna-\ntional Conference on Information and Knowledge Management (CIKM) , Association for\nComputing Machinery, 539\u2013548.\n[15] Lou, Y., Caruana, R., Gehrke, J., Hooker, G. (2013). Accurate intelligible models with\npairwise interactions. In Proceedings of the 19th ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining , Association for Computing Machinery, 623-631.\n[16] Lundberg, S.M. (2018). shap.PermutationExplainer. https://shap.readthedocs.io/en/\nlatest/generated/shap.PermutationExplainer.html\n[17] Lundberg, S.M., Lee, S.-I. (2017). A unified approach to interpreting model predictions. In:\nGuyon, I. Luxburg, U.V., Bengio, S., Wallach, H., Fergus, R. Vishwanathan, S., Garnett,\nR. (eds.), Advances in Neural Information Processing Systems 30, 4765-4774.\n22\n\n--- Page 23 ---\n[18] Maillart, T., Robert, C. (2024). Distilling Additive Tree Models into Generalized Linear\nModels: A Three-Step GAM Approach. In Annals of Actuarial Science , 18(3), 692\u2013711.\n[19] Mayer, M., Stando, A. (2025). shapviz : SHAP visualizations. Rpackage.\n[20] Mayer, M., W\u00a8 uthrich, M.V. (2025). Shapley values: paired-sampling algorithms.\narXiv :2508.12947.\n[21] McCullagh, P., Nelder, J.A. (1983). Generalized Linear Models . Chapman & Hall.\n[22] Nelder, J.A., Wedderburn, R.W.M. (1972). Generalized linear models. Journal of the Royal\nStatistical Society, Series A 135/3 , 370-384.\n[23] Richman, R. (2021). AI in actuarial science\u2013a review of recent advances\u2013part 1 . Annals of\nActuarial Science, 15(2) , 207\u2013229.\n[24] Richman, R. (2021). AI in actuarial science\u2013a review of recent advances\u2013part 2 . Annals of\nActuarial Science, 15(2) , 230\u2013258.\n[25] Richman, R., Scognamiglio, S., W\u00a8 uthrich, M.V. (2025). The credibility transformer. Euro-\npean Actuarial Journal , online first.\n[26] Shapley, L.S. (1953). A value for n-person games. In: Kuhn, H.W., Tucker, A.W. (eds.),\nContributions to the Theory of Games , AM-28, Volume II, Princeton University Press,\n307-318.\n[27] Song, W., Shi, C., Xiao, Z., Duan, Z., Xu, Y., Zhang, M., Tang, J. (2019). AutoInt:\nAutomatic feature interaction learning via self-attentive neural networks. Proceedings of the\n28th ACM International Conference on Information and Knowledge Management (CIKM) ,\n1161\u20131170.\n[28]\u02c7Strumbelj, E., Kononenko, I. (2010). An efficient explanation of individual classifications\nusing game theory. Journal of Machine Learning Research 11, 1-18.\n[29]\u02c7Strumbelj, E., Kononenko, I. (2014). Explaining prediction models and individual predic-\ntions with feature contributions. Knowledge and Information Systems 41/3 , 647-665.\n[30] Tsang, M., Cheng, D., Liu, Y. (2018). Detecting statistical interactions from neural network\nweights. In Proceedings of the 6th International Conference on Learning Representations\n(ICLR) , 1-21.\n[31] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,  L.,\nPolosukhin, I. (2017). Attention is all you need. arXiv :1706.03762v5.\n[32] Wood, S.N. (2017). Generalized Additive Models: an Introduction with R.2nd edition. CRC\nPress.\n[33] W\u00a8 uthrich, M.V., Merz, M. (2019). Editorial: Yes, we CANN! ASTIN Bulletin - The Journal\nof the IAA 49/1 , 1-3.\n23\n\n--- Page 24 ---\n[34] W\u00a8 uthrich, M.V., Merz, M. (2023). Statistical Foundations of Actuarial Learning\nand its Applications . Springer Actuarial. https://link.springer.com/book/10.1007/\n978-3-031-12409-9\n[35] W\u00a8 uthrich, M.V., Richman, R., Avanzi, B., Lindholm, M. Maggi, M., Mayer, M., Schelldor-\nfer, J., Scognamiglio, S. (2025). AI Tools for Actuaries. SSRN Manuscript ID 5162304.\n24",
  "project_dir": "artifacts/projects/enhanced_stat.ML_2508.15678v1_Tree_like_Pairwise_Interaction_Networks",
  "communication_dir": "artifacts/projects/enhanced_stat.ML_2508.15678v1_Tree_like_Pairwise_Interaction_Networks/.agent_comm",
  "assigned_at": "2025-08-22T20:58:05.213329",
  "status": "assigned"
}